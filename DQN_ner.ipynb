{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import math\n",
    "sys.path.insert(1, './src')\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "from game_ner import NERGame\n",
    "from robot import RobotCNNDQN\n",
    "import numpy as np\n",
    "import helpers\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tagger import CRFTagger\n",
    "import pickle\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--agent', help=\"require a decision agent\")\n",
    "    parser.add_argument('--episode', help=\"require a maximum number of playing the game\")\n",
    "    parser.add_argument('--budget', help=\"requrie a budget for annotating\")\n",
    "    parser.add_argument('--train', help=\"training phase\")\n",
    "    parser.add_argument('--test', help=\"testing phase\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    global AGENT, MAX_EPISODE, BUDGET, TRAIN_LANG, TEST_LANG\n",
    "    AGENT = args.agent\n",
    "    MAX_EPISODE = int(args.episode)\n",
    "    BUDGET = int(args.budget)\n",
    "    # load the train data: source languages\n",
    "    parts = args.train.split(\";\")\n",
    "    if len(parts) % 5 != 0:\n",
    "        print (\"Wrong inputs of training\")\n",
    "        raise SystemExit\n",
    "    global TRAIN_LANG_NUM\n",
    "    TRAIN_LANG_NUM = len(parts) / 5\n",
    "    for i in range(TRAIN_LANG_NUM):\n",
    "        lang_i = i * 5\n",
    "        train = parts[lang_i + 0]\n",
    "        test = parts[lang_i + 1]\n",
    "        dev = parts[lang_i + 2]\n",
    "        emb = parts[lang_i + 3]\n",
    "        tagger = parts[lang_i + 4]\n",
    "        TRAIN_LANG.append((train, test, dev, emb, tagger))\n",
    "    # load the test data: target languages\n",
    "    parts = args.test.split(\";\")\n",
    "    if len(parts) % 5 != 0:\n",
    "        print (\"Wrong inputs of testing\")\n",
    "        raise SystemExit\n",
    "    global TEST_LANG_NUM\n",
    "    TEST_LANG_NUM = len(parts) / 5\n",
    "    for i in range(TEST_LANG_NUM):\n",
    "        lang_i = i * 5\n",
    "        train = parts[lang_i + 0]\n",
    "        test = parts[lang_i + 1]\n",
    "        dev = parts[lang_i + 2]\n",
    "        emb = parts[lang_i + 3]\n",
    "        tagger = parts[lang_i + 4]\n",
    "        TEST_LANG.append((train, test, dev, emb, tagger))\n",
    "\n",
    "def initialise_game(train_file, test_file, dev_file, emb_file, budget):\n",
    "    # Load data\n",
    "    print(\"Loading data ..\")\n",
    "    train_x, train_y, train_lens = helpers.load_data2labels(train_file)\n",
    "    test_x, test_y, test_lens = helpers.load_data2labels(test_file)\n",
    "    dev_x, dev_y, dev_lens = helpers.load_data2labels(dev_file)\n",
    "\n",
    "    print(\"Processing data\")\n",
    "    # build vocabulary\n",
    "    max_len = FLAGS.flag_values_dict()['max_seq_len']\n",
    "    print (\"Max document length:\", max_len)\n",
    "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n",
    "        max_document_length=max_len, min_frequency=1)\n",
    "    # vocab = vocab_processor.vocabulary_ # start from {\"<UNK>\":0}\n",
    "    train_idx = np.array(list(vocab_processor.fit_transform(train_x)))\n",
    "    dev_idx = np.array(list(vocab_processor.fit_transform(dev_x)))\n",
    "    vocab = vocab_processor.vocabulary_\n",
    "    vocab.freeze()\n",
    "    test_idx = np.array(list(vocab_processor.fit_transform(test_x)))\n",
    "\n",
    "    # build embeddings\n",
    "    vocab = vocab_processor.vocabulary_\n",
    "    vocab_size = FLAGS.flag_values_dict()['max_vocab_size']\n",
    "    w2v = helpers.load_crosslingual_embeddings(emb_file, vocab, vocab_size)\n",
    "\n",
    "    # prepare story\n",
    "    story = [train_x, train_y, train_idx]\n",
    "    print (\"The length of the story \", len(train_x), \" ( DEV = \", len(dev_x), \" TEST = \", len(test_x), \" )\")\n",
    "    test = [test_x, test_y, test_idx]\n",
    "    dev = [dev_x, dev_y, dev_idx]\n",
    "    # load game\n",
    "    print(\"Loading game ..\")\n",
    "    game = NERGame(story, test, dev, max_len, w2v, budget)\n",
    "    return game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_ner():\n",
    "    actions = 2\n",
    "    global AGENT\n",
    "    robot = RobotCNNDQN(AGENT, actions)\n",
    "#     if AGENT == \"random\":\n",
    "#         robot = RobotRandom(actions)\n",
    "#     elif AGENT == \"DQN\":\n",
    "#         robot = RobotDQN(actions)\n",
    "#     elif AGENT == \"CNNDQN\":\n",
    "#         robot = RobotCNNDQN(actions)\n",
    "#     else:\n",
    "#         print (\"** There is no robot.\")\n",
    "#         raise SystemExit\n",
    "\n",
    "    global TRAIN_LANG, TRAIN_LANG_NUM, BUDGET\n",
    "    for i in range(TRAIN_LANG_NUM):\n",
    "        train = TRAIN_LANG[i][0]\n",
    "        test = TRAIN_LANG[i][1]\n",
    "        dev = TRAIN_LANG[i][2]\n",
    "        emb = TRAIN_LANG[i][3]\n",
    "        tagger = TRAIN_LANG[i][4]\n",
    "        # initilise a NER game\n",
    "        game = initialise_game(train, test, dev, emb, BUDGET)\n",
    "        # initialise a decision robot\n",
    "        robot.initialise(game.max_len, game.w2v)\n",
    "#         robot.update_embeddings(game.w2v)\n",
    "        # tagger\n",
    "        model = CRFTagger(tagger)\n",
    "        # play game\n",
    "        episode = 1\n",
    "        print(\">>>>>> Playing game ..\")\n",
    "        while episode <= MAX_EPISODE:\n",
    "            print ('>>>>>>> Current game round ', episode, 'Maximum ', MAX_EPISODE)\n",
    "            observation = game.get_frame(model)\n",
    "            print (observation[1])\n",
    "            print (observation[3])\n",
    "            action = robot.get_action(observation)\n",
    "            print ('> Action', action)\n",
    "            reward, observation2, terminal = game.feedback(action, model)\n",
    "            print ('> Reward', reward)\n",
    "            robot.update(observation, action, reward, observation2, terminal)\n",
    "            if terminal == True:\n",
    "                episode += 1\n",
    "                print ('> Terminal <')\n",
    "    return (robot, game, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_batch(robot, game, model, budget):\n",
    "    i = 0\n",
    "    queried_x = []\n",
    "    queried_y = []\n",
    "    performance = []\n",
    "    cost = []\n",
    "    test_sents = helpers.data2sents(game.test_x, game.test_y)\n",
    "    game.replay()\n",
    "    while i < budget:\n",
    "        sel_ind = game.current_frame\n",
    "        # construct the observation\n",
    "        observation = game.get_frame(model)\n",
    "        action = robot.get_action(observation)\n",
    "        if action[1] == 1:\n",
    "            sentence = game.train_x[sel_ind]\n",
    "            labels = game.train_y[sel_ind]\n",
    "            queried_x.append(sentence)\n",
    "            queried_y.append(labels)\n",
    "            i += 1\n",
    "            train_sents = helpers.data2sents(queried_x, queried_y)\n",
    "            model.train(train_sents)\n",
    "            performance.append(model.test(test_sents))\n",
    "            cost.append(i)\n",
    "        game.current_frame += 1\n",
    "    print ('***COST', cost)\n",
    "    print (\"***TEST\", performance)\n",
    "    return (cost, performance)\n",
    "\n",
    "def test_agent_online(robot, game, model, budget):\n",
    "    # to address game -> we have a new game here\n",
    "    i = 0\n",
    "    queried_x = []\n",
    "    queried_y = []\n",
    "    performance = []\n",
    "    test_sents = helpers.data2sents(game.test_x, game.test_y)\n",
    "    game.reboot()\n",
    "    while i < budget:\n",
    "        sel_ind = game.current_frame\n",
    "        # construct the observation\n",
    "        observation = game.get_frame(model)\n",
    "        action = robot.get_action(observation)\n",
    "        if action[1] == 1:\n",
    "            sentence = game.train_x[sel_ind]\n",
    "            labels = game.train_y[sel_ind]\n",
    "            queried_x.append(sentence)\n",
    "            queried_y.append(labels)\n",
    "            i += 1\n",
    "            train_sents = helpers.data2sents(queried_x, queried_y)\n",
    "            model.train(train_sents)\n",
    "            performance.append(model.test(test_sents))\n",
    "\n",
    "        reward, observation2, terminal = game.feedback(action, model)  # game\n",
    "        robot.update(observation, action, reward, observation2, terminal)\n",
    "    # train a crf and evaluate it\n",
    "    train_sents = helpers.data2sents(queried_x, queried_y)\n",
    "    model.train(train_sents)\n",
    "    performance.append(model.test(test_sents))\n",
    "    print (\"***TEST\", performance)\n",
    "\n",
    "def test(robot):\n",
    "    global TEST_LANG, TEST_LANG_NUM, BUDGET\n",
    "    for i in range(TEST_LANG_NUM):\n",
    "        train = TEST_LANG[i][0]\n",
    "        test = TEST_LANG[i][1]\n",
    "        dev = TEST_LANG[i][2]\n",
    "        emb = TEST_LANG[i][3]\n",
    "        tagger = TEST_LANG[i][4]\n",
    "        game2 = initialise_game(train, test, dev, emb, BUDGET)\n",
    "        robot.update_embeddings(game2.w2v)\n",
    "        model = CRFTagger(tagger)\n",
    "        test_agent_batch(robot, game2, model, BUDGET)\n",
    "        test_agent_online(robot, game2, model, BUDGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "LOGTOSTDERR=False\n",
      "ALSOLOGTOSTDERR=False\n",
      "LOG_DIR=\n",
      "V=-1\n",
      "VERBOSITY=-1\n",
      "STDERRTHRESHOLD=fatal\n",
      "SHOWPREFIXFORINFO=True\n",
      "RUN_WITH_PDB=False\n",
      "PDB_POST_MORTEM=False\n",
      "RUN_WITH_PROFILING=False\n",
      "PROFILE_FILE=None\n",
      "USE_CPROFILE_FOR_PROFILING=True\n",
      "ONLY_CHECK_ARGS=False\n",
      "OP_CONVERSION_FALLBACK_TO_WHILE_LOOP=False\n",
      "TEST_RANDOM_SEED=301\n",
      "TEST_SRCDIR=\n",
      "TEST_TMPDIR=/tmp/absl_testing\n",
      "TEST_RANDOMIZE_ORDERING_SEED=None\n",
      "XML_OUTPUT_FILE=\n",
      "MAX_SEQ_LEN=120\n",
      "MAX_VOCAB_SIZE=20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flags = tf.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer(\"max_seq_len\", 120, \"sequence\")\n",
    "flags.DEFINE_integer(\"max_vocab_size\", 20000, \"vocabulary\")\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in FLAGS.flag_values_dict().items():\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_args()\n",
    "global AGENT, MAX_EPISODE, BUDGET, TRAIN_LANG, TEST_LANG, TRAIN_LANG_NUM, TEST_LANG_NUM\n",
    "SOURCE = 'conll'\n",
    "AGENT = 'CNNDQN'\n",
    "MAX_EPISODE = 1\n",
    "BUDGET = 5\n",
    "\n",
    "DATAPATH = './datasets/{}'.format(SOURCE)\n",
    "train_f  = DATAPATH + '.train'\n",
    "test_f   = DATAPATH + '.test'\n",
    "dev_f    = DATAPATH + '.dev'\n",
    "emb_f    = DATAPATH + 'NUM.kv'\n",
    "tagger_f = DATAPATH + 'model.saved'\n",
    "\n",
    "TRAIN_LANG = []\n",
    "TEST_LANG = []\n",
    "TRAIN_LANG_NUM = 1\n",
    "TRAIN_LANG.append((train_f, test_f, dev_f, emb_f, tagger_f))\n",
    "TEST_LANG_NUM = 1\n",
    "TEST_LANG.append((train_f, test_f, dev_f, emb_f, tagger_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0930 07:54:05.882400 140714725021504 deprecation_wrapper.py:119] From ./src/robot.py:202: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0930 07:54:05.901150 140714725021504 deprecation_wrapper.py:119] From ./src/robot.py:212: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0930 07:54:05.910346 140714725021504 deprecation_wrapper.py:119] From ./src/robot.py:224: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0930 07:54:05.919106 140714725021504 deprecation_wrapper.py:119] From ./src/robot.py:237: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0930 07:54:05.939310 140714725021504 deprecation.py:506] From ./src/robot.py:250: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0930 07:54:05.956245 140714725021504 deprecation_wrapper.py:119] From ./src/robot.py:283: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "W0930 07:54:05.995305 140714725021504 deprecation_wrapper.py:119] From ./src/robot.py:95: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a robot: CNN-DQN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0930 07:54:06.225312 140714725021504 deprecation_wrapper.py:119] From ./src/robot.py:97: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0930 07:54:06.791177 140714725021504 deprecation_wrapper.py:119] From ./src/robot.py:99: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "W0930 07:54:06.876793 140714725021504 deprecation_wrapper.py:119] From ./src/robot.py:38: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ..\n",
      "Processing data\n",
      "Max document length: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0930 07:54:08.852131 140714725021504 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0930 07:54:08.853000 140714725021504 deprecation.py:323] From <ipython-input-2-9d0fe32ac77c>:57: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "W0930 07:54:08.853412 140714725021504 deprecation.py:323] From /zf18/ll5fy/miniconda3/envs/ll5fy36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "W0930 07:54:08.853781 140714725021504 deprecation.py:323] From /zf18/ll5fy/miniconda3/envs/ll5fy36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary 1339 Embedding size 200\n",
      "The length of the story  615  ( DEV =  200  TEST =  200  )\n",
      "Loading game ..\n",
      "Initilizing the game:\n",
      "Story: length =  615\n",
      "CRF Tagger\n",
      ">>>>>> Playing game ..\n",
      ">>>>>>> Current game round  1 Maximum  1\n",
      "[0.8608480576077683]\n",
      "[5]\n",
      "DQN is smart.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1b6b715ad0df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# play games for training a robot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrobot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcost_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_agent_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrobot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBUDGET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0macc_valid_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f4fadfd56e80>\u001b[0m in \u001b[0;36mplay_ner\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'> Action'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/net/zf18/ll5fy/dnn/active-learning/PAL/src/robot.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# print sent, confidence, predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         qvalue = self.sess.run(self.qvalue, feed_dict={self.sent: [\n\u001b[0;32m--> 167\u001b[0;31m                                sent], self.state_confidence: [confidence], self.predictions: [predictions]})[0]\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ll5fy36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ll5fy36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/miniconda3/envs/ll5fy36/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# play games for training a robot\n",
    "robot, game, model = play_ner()\n",
    "cost_list, acc_list = test_agent_batch(robot, game, model, BUDGET)\n",
    "acc_valid_list = acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/{}-{}.bin\".format(SOURCE, MAX_EPISODE), \"wb\") as result:\n",
    "    pickle.dump((cost_list, acc_list, acc_valid_list), result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/conll-20.bin\", \"rb\") as in_file:\n",
    "    (cost_1, acc_1, acc_valid_1) = pickle.load(in_file)\n",
    "print (cost_1)\n",
    "print (acc_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim beta\n",
    "with open(\"./results/CNNDQN-conll-20.bin\", \"rb\") as in_file:\n",
    "    (cost_1, all_acc_1, acc_valid_1) = pickle.load(in_file)\n",
    "#     (cost_1, all_acc_1, acc_valid_1) = pickle.load(in_file)\n",
    "       \n",
    "with open(\"./results/CNNDQN1-conll-20.bin\", \"rb\") as in_file:\n",
    "    (cost_2, all_acc_2, acc_valid_2) = pickle.load(in_file)\n",
    "\n",
    "with open(\"./results/CNNDQN2-conll-20.bin\", \"rb\") as in_file:\n",
    "    (cost_3, all_acc_3, acc_valid_3) = pickle.load(in_file)\n",
    "\n",
    "with open(\"./results/CNNDQN3-conll-20.bin\", \"rb\") as in_file:\n",
    "    (cost_4, all_acc_4, acc_valid_4) = pickle.load(in_file)\n",
    "    \n",
    "with open(\"../active-RL/results/conll15_200_75budget_fully_none.bin\", \"rb\") as in_file:\n",
    "    (cost_5, _, _, all_acc_5, acc_valid_5) = pickle.load(in_file)\n",
    "\n",
    "# with open(\"./results/CNNDQN_2-conll-20.bin\", \"rb\") as in_file:\n",
    "#     (cost_6, all_acc_6, acc_valid_6) = pickle.load(in_file)\n",
    "\n",
    "# with open(\"./results/CNNDQN_3-conll-20.bin\", \"rb\") as in_file:\n",
    "#     (cost_7, all_acc_7, acc_valid_7) = pickle.load(in_file)\n",
    "\n",
    "plt.plot(cost_1[15:], all_acc_1[15:],\n",
    "         cost_2[15:], all_acc_2[15:],\n",
    "         cost_3[15:], all_acc_3[15:],\n",
    "         cost_4[15:], all_acc_4[15:],\n",
    "         cost_5, all_acc_5,)\n",
    "#          cost_6[15:], all_acc_6[15:],\n",
    "#          cost_7[15:], all_acc_7[15:])\n",
    "plt.legend(['all', '1', '2', '3', 'TE' ,'-2', '-3'], loc='upper left', fancybox=True, fontsize = 9)\n",
    "# plt.xlim(200, 600)\n",
    "\n",
    "plt.title('CoNLL dataset')\n",
    "plt.xlabel('Number of training sequences')\n",
    "plt.ylabel('Prediction accuracy')\n",
    "plt.savefig('./results/sod.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "ll5fy36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
